{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "p0kP1FMpVS64",
        "outputId": "0f78953f-a491-4de9-ceab-8286e199f6ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make langchain demo \n",
        "%cd /content/drive/MyDrive/models\n",
        "#如果模型已经下载，不需要执行\n",
        "# !apt-get install git-lfs=2.13.3\n",
        "# !pip install transformers\n",
        "# %cd /content/drive/MyDrive/models \n",
        "# !git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese \n",
        "# %cd /content/drive/MyDrive/models \n",
        "# !git clone https://huggingface.co/rabitt/Chinese-Alpaca-Plus-13B-GPTQ\n",
        "# %cd /content/drive/MyDrive/models \n",
        "# !git clone https://huggingface.co/THUDM/chatglm-6b-int4\n",
        "#!git clone https://huggingface.co/rabitt/Chinese-Alpaca-Plus-13B-GPTQ\n",
        "#!pip install protobuf==3.20.0\n",
        "\n",
        "#!git clone https://huggingface.co/Chinese-Vicuna/llama7b_4bit_128g\n",
        "!git clone https://huggingface.co/decapoda-research/llama-7b-hf\n",
        "!git clone https://huggingface.co/Chinese-Vicuna/Chinese-Vicuna-lora-7b-belle-and-guanaco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vIc-vmd4ejM",
        "outputId": "0c6feeb6-d1f0-46f2-dbf9-3c8ba608910b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/models\n",
            "Cloning into 'llama-7b-hf'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
            "Unpacking objects: 100% (91/91), 22.58 KiB | 25.00 KiB/s, done.\n",
            "Downloading tokenizer.model (500 KB)\n",
            "Error downloading object: tokenizer.model (9e556af): Smudge error: Error downloading tokenizer.model (9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347): read /content/drive/MyDrive/models/llama-7b-hf/.git/lfs/incomplete/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347778023588: no such file or directory\n",
            "\n",
            "Errors logged to /content/drive/MyDrive/models/llama-7b-hf/.git/lfs/logs/20230604T160328.933343555.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: tokenizer.model: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "Cloning into 'Chinese-Vicuna-lora-7b-belle-and-guanaco'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Total 25 (delta 0), reused 0 (delta 0), pack-reused 25\u001b[K\n",
            "Unpacking objects: 100% (25/25), 21.06 KiB | 78.00 KiB/s, done.\n",
            "Downloading adapter_model.bin (17 MB)\n",
            "Error downloading object: adapter_model.bin (cd27e3e): Smudge error: Error downloading adapter_model.bin (cd27e3e822af21deaa693661a25492e873a5ffc3f8a4784b68fca4bd1d0f5c5c): read /content/drive/MyDrive/models/Chinese-Vicuna-lora-7b-belle-and-guanaco/.git/lfs/incomplete/cd27e3e822af21deaa693661a25492e873a5ffc3f8a4784b68fca4bd1d0f5c5c306854350: no such file or directory\n",
            "\n",
            "Errors logged to /content/drive/MyDrive/models/Chinese-Vicuna-lora-7b-belle-and-guanaco/.git/lfs/logs/20230604T160330.536494848.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: adapter_model.bin: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "# !git clone https://github.com/hwchase17/langchain\n",
        "# !cd langchain\n",
        "# !pip install -e . \n",
        "# !pip install sentence_transformers faiss-cpu\n",
        "#!git clone https://github.com/imClumsyPanda/langchain-ChatGLM.git\n",
        "%cd langchain-ChatGLM\n",
        "#!pip install -r requirements.txt \n",
        "!python webui.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEldgkMioM64",
        "outputId": "186ad09a-5041-41e7-cfa7-c07c1e7ecb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/langchain-ChatGLM\n",
            "INFO  2023-06-04 15:26:56,735-1d: \n",
            "loading model config\n",
            "llm device: cuda\n",
            "embedding device: cuda\n",
            "dir: /content/drive/MyDrive/langchain-ChatGLM\n",
            "flagging username: 9e4576f656474fc5841fc62fabd6507b\n",
            "\n",
            "Loading alpaca-13b-cn...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/langchain-ChatGLM/webui.py\", line 279, in <module>\n",
            "    model_status = init_model()\n",
            "  File \"/content/drive/MyDrive/langchain-ChatGLM/webui.py\", line 105, in init_model\n",
            "    llm_model_ins = shared.loaderLLM()\n",
            "  File \"/content/drive/MyDrive/langchain-ChatGLM/models/shared.py\", line 40, in loaderLLM\n",
            "    loaderCheckPoint.reload_model()\n",
            "  File \"/content/drive/MyDrive/langchain-ChatGLM/models/loader/loader.py\", line 402, in reload_model\n",
            "    self.model, self.tokenizer = self._load_model(self.model_name)\n",
            "  File \"/content/drive/MyDrive/langchain-ChatGLM/models/loader/loader.py\", line 108, in _load_model\n",
            "    LoaderClass.from_pretrained(checkpoint,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 467, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2387, in from_pretrained\n",
            "    raise EnvironmentError(\n",
            "OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /content/drive/MyDrive/models/Chinese-Alpaca-Plus-13B-GPTQ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# webui方式"
      ],
      "metadata": {
        "id": "Xqf1MAZYTuwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "!git clone https://github.com/oobabooga/text-generation-webui.git\n",
        "%cd ./text-generation-webui\n",
        "!pip install -r requirements.txt "
      ],
      "metadata": {
        "id": "MXLqqaXO6FwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWIEIapOAstd"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/drive/MyDrive/text-generation-webui/repositories\n",
        "%cd /content/drive/MyDrive/text-generation-webui/repositories\n",
        "!git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\n",
        "%cd GPTQ-for-LLaMa\n",
        "!python setup_cuda.py install\n",
        "\n",
        "drive_folder = '/content/drive/MyDrive/drive/MyDrive/text-generation-webui'\n",
        "if not os.path.exists(drive_folder):\n",
        "    os.makedirs(drive_folder)\n",
        "!mv /content/drive/MyDrive/text-generation-webui/* \"$drive_folder/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEGdDI1SED4j"
      },
      "outputs": [],
      "source": [
        "!apt-get -y install -qq aria2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mht9fpviBonB"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "def download_file(url, path, filename):\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {url} -d {path} -o {filename}\"\n",
        "    print(f\"Downloading {filename}...\")\n",
        "    subprocess.run(command, shell=True)\n",
        "    print(f\"{filename} downloaded successfully.\")\n",
        "  \n",
        "model = \"chinese-alpaca\"  # Set this to \"alpaca\" to select the alpaca model\n",
        "if model == \"alpaca\":\n",
        "    base_url = \"https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\"\n",
        "    safetensors = \"4bit-128g.safetensors\"\n",
        "elif model == \"vicuna\":\n",
        "    base_url = \"https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g\"\n",
        "    safetensors = \"vicuna-13b-4bit-128g.safetensors\"\n",
        "elif model ==\"chinese-alpaca\":\n",
        "  #ref https://github.com/ymcui/Chinese-LLaMA-Alpaca#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD 模型下载说明\n",
        "    base_url=\"https://huggingface.co/rabitt/Chinese-Alpaca-Plus-13B-GPTQ\"#\n",
        "    safetensors = \"Chinese-Alpaca-Plus-13B-GPTQ-4bit-128g.safetensors\"\n",
        "elif model ==\"chinese-vicuna\":\n",
        "  #https://github.com/Facico/Chinese-Vicuna\n",
        "    base_url=\"https://huggingface.co/Chinese-Vicuna/llama7b_4bit_128g/tree/main\" # 结合llama7b use   \n",
        "else:\n",
        "    raise ValueError(\"Invalid model selected\")\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/text-generation-webui/models/gpt4-x-\" + model + \"-13b-native-4bit-128g\"\n",
        "\n",
        "download_file(f\"{base_url}/raw/main/config.json\", base_path, \"config.json\")\n",
        "download_file(f\"{base_url}/raw/main/generation_config.json\", base_path, \"generation_config.json\")\n",
        "download_file(f\"{base_url}/raw/main/special_tokens_map.json\", base_path, \"special_tokens_map.json\")\n",
        "download_file(f\"{base_url}/resolve/main/tokenizer.model\", base_path, \"tokenizer.model\")\n",
        "download_file(f\"{base_url}/raw/main/tokenizer_config.json\", base_path, \"tokenizer_config.json\")\n",
        "download_file(f\"{base_url}/resolve/main/{safetensors}\", base_path, safetensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1157a1yBpq4"
      },
      "outputs": [],
      "source": [
        "import os  \n",
        "\n",
        "%cd /content/drive/MyDrive/text-generation-webui\n",
        "\n",
        "auth_token_path = \"/content/drive/MyDrive/text-generation-webui/auth_token.txt\"\n",
        "\n",
        "# Set your username and password here (or use the default)\n",
        "user = \"username\" \n",
        "password = \"password\"\n",
        " \n",
        "auth = \"no\"#input(\"Do you want to enable authentication? (yes/no): \") \n",
        "\n",
        "if auth.lower() == \"yes\":\n",
        "    with open(auth_token_path, \"w\") as f:\n",
        "        f.write(f\"{user}:{password}\")\n",
        "    print(\"auth_token.txt created successfully.\") \n",
        "\n",
        "if os.path.exists(auth_token_path):\n",
        "    !python server.py  --model_type llama --share --chat --wbits 4 --groupsize 128 --gradio-auth-path {auth_token_path}\n",
        "else:\n",
        "    !python server.py  --model_type llama --share --chat --wbits 4 --groupsize 128"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}